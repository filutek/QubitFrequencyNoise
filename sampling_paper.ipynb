{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.axes_grid1.inset_locator import (inset_axes, InsetPosition,\n",
    "                                                  mark_inset)\n",
    "from scipy.stats import binned_statistic_2d\n",
    "import time as time\n",
    "from scipy.stats import binom, skew\n",
    "import pickle\n",
    "from scipy.optimize import curve_fit\n",
    "from matplotlib.cm import get_cmap\n",
    "from scipy.fft import fft, ifft\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import ast\n",
    "import os\n",
    "from decimal import Decimal \n",
    "import scipy.linalg as scl\n",
    "from numpy.linalg import det\n",
    "from scipy.special import sici\n",
    "from scipy.integrate import quad as integrate\n",
    "import itertools \n",
    "from scipy.signal import periodogram as PS\n",
    "from scipy.signal import welch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for saving and representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_label_func(phi):\n",
    "    if phi == np.pi:\n",
    "        phi_label = r'$\\pi$'\n",
    "    elif phi == np.pi/2:\n",
    "        phi_label = r'$\\frac{\\pi}{2}$'\n",
    "    elif phi == np.pi/4:\n",
    "        phi_label = r'$\\frac{\\pi}{4}$'\n",
    "        \n",
    "    elif phi == np.pi/3:\n",
    "        phi_label = r'$\\frac{\\pi}{3}$'\n",
    "        \n",
    "    elif phi == 3*np.pi/4:a\n",
    "        phi_label = r'$\\frac{3\\pi}{4}$'\n",
    "    elif phi == 0:\n",
    "        phi_label = '0'\n",
    "    else:\n",
    "        phi_label = str(np.round(phi,2))\n",
    "    return phi_label\n",
    "\n",
    "def create_spec_dict(T=1,t_seq = 3, lag = 3, N = 100000, omega = 0.01, D=1, NTLS = 10,\n",
    "                    alpha = 0.75, W=0.01, V = 1, reps = 200, phi = np.pi/4):\n",
    "    specs_dict = {'T' : T, 't_seq' : t_seq, 'lag' : lag, 'N' : N, 'reps' : reps, 'NTLS' : NTLS,\n",
    "                 'D' : D, 'omega' : omega, 'alpha' : alpha, 'W' : W, 'V' : V, 'phi' : phi}\n",
    "    return specs_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining correlation functions. For the centered correlation function we use the following definition:\n",
    "\n",
    "For an array of outcomes $x$ of size ($m, M$) (for simplicity let's assume that it's a 1-D array, so we use $m=1$)\n",
    "\n",
    "$$\n",
    "C(k) =  \\left\\langle (x[:M-k] - \\langle x[:M-k]\\rangle) (x[k:]-\\langle x[k:]\\rangle)\\right\\rangle,\n",
    "$$\n",
    "where \n",
    "$$\\langle x[:M-k]\\rangle := \\frac{1}{M-k}\\sum_{i=0}^{M-k-1} x_i,$$\n",
    "is an average over all outcomes of the sublist defined by the indices. Note, that the average in the first component, and the second one are different, and both are different than the total average, i.e. $r_1$. This allows us to get rid of statistical biases caused by a finite length of the arrays.\n",
    "For 2-D arrays, we do the same, but the average is taken for each repetition. This is done in parallel through vectorization procedure. \n",
    "Next, $C(k)$ is aranged into a list [$C(k)$ for k in range(1,N)], where $N$ is the terminanting value of the list, that in princple can be as big as $M$. However, to avoid statistical uncertaintites, it is usually taken to be $N\\le 1/\\sqrt{M}$. Such composed list is what we call $\\tilde{r}_2(k)$ in the paper. \n",
    "\n",
    "Analogously, we define three-time correlator $C(k,l)$ (i.e. $\\tilde{r}_3(k,l)$), but instead of having two lists, we use three of them, and fix either $k$ or $l$ index to iterate over just one (for efficiency). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(output, N = 0):\n",
    "    output = np.array(output)\n",
    "    if N == 0:\n",
    "        N = output.shape[-1] \n",
    "    M = output.shape[-1] \n",
    "    m = output.shape[0]\n",
    "    \n",
    "    \n",
    "    if len(output.shape) == 1:\n",
    "        \n",
    "        C_out = [np.mean((output[:M-k]- np.mean(output[:M-k])) * (output[k: ]- np.mean(output[k:]))) for k in range(N)]\n",
    "    else:\n",
    "        pi_1 = np.reshape(np.mean(output, axis = 1), (m,1))\n",
    "        C_out = [np.mean((output[:,:M-k]- np.reshape(np.mean(output[:,:M-k], axis = 1), (m,1)) ) * \n",
    "                        (output[:,k: ]-  np.reshape(np.mean(output[:,k:], axis = 1), (m,1))), axis = 1) for k in range(N)]\n",
    "    return C_out\n",
    "\n",
    "\n",
    "def correlation_3_shift(output, lag, lag_type = 'k', N = 0):\n",
    "    output = np.array(output)\n",
    "    \n",
    "    if N == 0:\n",
    "        N = output.shape[-1] \n",
    "    M = output.shape[-1] \n",
    "    m = output.shape[0]\n",
    "\n",
    "    tin = time.monotonic()\n",
    "    if len(output.shape) == 1:\n",
    "        if lag_type == 'k':\n",
    "            C_out = [np.mean((output[:-k-lag]-np.mean(output[:-k-lag])) * \n",
    "                             (output[k:-lag ]-np.mean(output[k:-lag ])) * \n",
    "                             (output[k+lag: ]-np.mean(output[k+lag: ])))\n",
    "                     for k in range(1,N)]\n",
    "        elif lag_type == 'l':\n",
    "            C_out = [np.mean((output[:-lag-k]) * (output[lag:-k]) * (output[lag+k: ])) \n",
    "                     for k in range(1,N)]   \n",
    "\n",
    "    else:\n",
    "        if lag_type == 'k':\n",
    "            C_out = [np.mean((output[:,:-k-lag]-np.reshape(np.mean(output[:,:-k-lag], axis = 1), (m,1))) * \n",
    "                             (output[:,k:-lag ]-np.reshape(np.mean(output[:,k:-lag ], axis = 1), (m,1))) * \n",
    "                             (output[:,k+lag: ]-np.reshape(np.mean(output[:,k+lag: ], axis = 1), (m,1))), axis = 1) \n",
    "                     for k in range(1,N)] \n",
    "        elif lag_type == 'l':\n",
    "            C_out = [np.mean((output[:,:-(k+lag)]-np.reshape(np.mean(output[:,:-(k+lag)], axis = 1), (m,1))) * \n",
    "                             (output[:,lag:-k]-np.reshape(np.mean(output[:,lag:-k], axis = 1), (m,1))) * \n",
    "                             (output[:,lag+k: ]-np.reshape(np.mean(output[:,lag+k:], axis = 1), (m,1))), axis = 1) \n",
    "                     for k in range(1,N-lag)]    \n",
    "        C_out = np.array(C_out).T\n",
    "    return C_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for simulation of TLSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_asymmetric(N, W12_list, W21_list, T, t_cycle,   dt, V_list, reps):\n",
    "    '''\n",
    "    \n",
    "    Input:\n",
    "    N       - number of repetitions of the experiment - Ramsey cycles\n",
    "    W12     - noise/correlation parameter characterizing transition from state 1 --> 2 (in paper 1->0)\n",
    "    W21     - as above, but the transition from 2 --> 1  (in paper 0->1)\n",
    "    T       - time between Ramsey pulses (in paper t_R)\n",
    "    t_cycle - total time for a single measurement (T + measurement and rest time) (in paper t_cyc)\n",
    "    dt      - the minimal time increment, in which flipping between state 1 and 2 is happening \n",
    "\n",
    "    Return: an array of TLS fluctuation outcomes: \"-V_n\" and \"+V_n\"\n",
    "    '''\n",
    "    W12_array = np.array(W12_list)\n",
    "    W21_array = np.array(W21_list)\n",
    "    W = W12_array + W21_array\n",
    "    \n",
    "    n = len(W12_list)\n",
    "    V_arr = np.reshape(np.array(V_list), (n,1))\n",
    "    \n",
    "    #random initial state either +1 or -1\n",
    "    dn = np.zeros((reps, n, int(t_cycle * N/dt)))  #this might need to be adjusted\n",
    "    dn[:,:,0] = (np.random.randint(2, size = (reps, n))*2 -1)\n",
    "    \n",
    "    #switching probabilities for the smallest time increments\n",
    "    p12 = W12_array * dt\n",
    "    p21 = W21_array * dt\n",
    "    \n",
    "    flip = 0\n",
    "    for idx in range(int(t_cycle * N/dt) -1):\n",
    "#         print('idx ', idx)\n",
    "        rnd = np.random.rand(reps,n)\n",
    "        I12 = (-2*(rnd < p12) +1)\n",
    "        I21 = (-2*(rnd < p21) +1)\n",
    "#         print(f'I12 : {I12}, I21 : {I21}')\n",
    "        d21 = dn[:,:,idx] == +1\n",
    "        d12 = dn[:,:,idx] == -1\n",
    "        dn[:,:,idx+1] = (-(d12 * I12) + d21 * I21)\n",
    "\n",
    "    return dn*V_arr\n",
    "\n",
    "\n",
    "def integrate_TLS(outcome_arr, N, T, t_cycle, dt):\n",
    "    '''\n",
    "    Function that transforms TLS fluctuations into a random phase theta, through sumation\n",
    "    \n",
    "    Input:\n",
    "    outcome_arr - results from TLS fluctuators, e.g. output of multiple_asymmetric_list function\n",
    "    N - number of Ramsey cycle\n",
    "    T       - time between Ramsey pulses\n",
    "    t_cycle - total time for a single measurement (T + measurement and rest time)\n",
    "    dt      - the minimal time increment, in which flipping between state 1 and 2 is happening\n",
    "    \n",
    "    Return: an array of phases, that allow us to define probability of an outcome\n",
    "    '''\n",
    "    \n",
    "    outcome_arr = np.array(outcome_arr)\n",
    "#     print('shape : ', outcome_arr.shape)\n",
    "    t_steps = int(t_cycle/dt)\n",
    "    theta = [dt *  np.sum(outcome_arr[:, :,k * t_steps: k*t_steps + int(T/dt)], axis = (1,2))\n",
    "             for k in range(N) ]\n",
    "    return np.array(theta).T\n",
    "\n",
    "\n",
    "def outcomes_measurements(theta_array, phi = 0):\n",
    "    '''\n",
    "    function that computes outcomes of the measurements based on the random thetas\n",
    "    '''\n",
    "    dim_0 = theta_array.shape[0]\n",
    "    dim_1 = theta_array.shape[1]\n",
    "    rnd = np.random.rand(dim_0, dim_1)\n",
    "\n",
    "    prob = 1/2 * (1 + np.cos(phi + theta_array))\n",
    "    return (prob >= rnd) *1\n",
    "\n",
    "\n",
    "def noise_stat_freq(N_total, W12_list, W21_list,T, t_cycle, dt, V_list, reps, phi = 0, avg = True):\n",
    "    '''\n",
    "    function that aggregates statistics of the outcomes\n",
    "    It allows us to compute multiple repetetion of the procedure (parameter reps)\n",
    "    \n",
    "    '''\n",
    "    dn = multiple_asymmetric(N = N_total, W12_list = W12_list, W21_list=W21_list, \n",
    "                               T = T, t_cycle = t_cycle, dt = dt, V_list = V_list, reps = reps)\n",
    "    theta = integrate_TLS(dn, N=N_total ,T= T, t_cycle = t_cycle,  dt=dt)\n",
    "    W, dW = delta_W(W12_list, W21_list)\n",
    "    V_arr = np.array(V_list)\n",
    "    theta_shift = T* np.sum(V_arr*dW/W)\n",
    "    outcomes_01 = outcomes_measurements(theta, phi = phi - theta_shift)\n",
    "    if avg:\n",
    "        dn = np.sum(dn, axis = 1)\n",
    "    return theta, outcomes_01, dn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to calculate $r_1$ and $r_2(k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_W(W12_list, W21_list):\n",
    "    '''\n",
    "    Calculates W = W_12 + W_21, and  ΔW = W_12 - W_21\n",
    "    Note that 2 is 0 papers notation and 1 is 1 (the change is due to old convention) \n",
    "    '''\n",
    "    W12_arr = np.array(W12_list)\n",
    "    W21_arr = np.array(W21_list)\n",
    "    dW_arr = W12_arr - W21_arr\n",
    "    W_arr = W12_arr + W21_arr\n",
    "    return W_arr, dW_arr\n",
    "\n",
    "def delta_nu(W12_list, W21_list, V_list):\n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "    V_arr = np.array(V_list)\n",
    "    nu = 1/2 * np.sqrt(W_arr**2 + 4j*V_arr*(dW_arr +1j*V_arr))\n",
    "    return nu\n",
    "\n",
    "def pi1_TLS(W12_list, W21_list, T, V_list, phi = 0):\n",
    "    '''\n",
    "    This is what in the paper is called $r_1$\n",
    "    '''\n",
    "    V_arr = np.array(V_list)\n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "    K_arr = dW_arr/W_arr\n",
    "    nu = delta_nu(W12_list, W21_list, V_list)\n",
    "    re = np.exp(-W_arr*T/2)/nu * ((W_arr/2 + 1j*V_arr*K_arr)*np.sinh(nu*T)+nu*np.cosh(nu*T))\n",
    "    return 1/2 + 1/2 * np.real(np.exp(1j*phi)*np.prod(re))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C_plus0(W12_list, W21_list, T, t_cycle, V_list, k=1):\n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "    nu = delta_nu(W12_list, W21_list, V_list)\n",
    "    V_arr = np.array(V_list)\n",
    "    return 1j*V_arr/(nu)*np.exp(-W_arr*(k*t_cycle - T/2))*np.sinh(nu*T)\n",
    "\n",
    "def C_plus0T(W12_list, W21_list, T, V_list):\n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "    nu = delta_nu(W12_list, W21_list, V_list)   \n",
    "    V_arr = np.array(V_list)\n",
    "    K_arr = dW_arr/W_arr \n",
    "    Q = (W_arr/2 + 1j*V_arr*K_arr)*np.sinh(nu*T)+nu*np.cosh(nu*T)\n",
    "    return 1/(2*nu)*np.exp(-W_arr*T/2)*Q\n",
    "    \n",
    "def J_plus(W12_list, W21_list, T, V_list):\n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "    nu = delta_nu(W12_list, W21_list, V_list) \n",
    "    V_arr = np.array(V_list)\n",
    "    K_arr = dW_arr/W_arr \n",
    "    W12_arr = np.array(W12_list)\n",
    "    W21_arr = np.array(W21_list)\n",
    "    return 2j*V_arr/nu * W12_arr*W21_arr/(W_arr**2) * np.exp(-W_arr*T/2)*np.sinh(nu*T)\n",
    "\n",
    "def K_ms(W12_list, W21_list, T, t_cycle, V_list, idx = 0, k=1):\n",
    "    J_p = J_plus(W12_list, W21_list, T, V_list)[:idx+1]\n",
    "    C_p = 2*C_plus0T(W12_list, W21_list, T, V_list)[idx+1:]\n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "    decay = np.exp(-np.sum(W_arr)*(k*t_cycle - T))\n",
    "    return (np.prod(J_p*C_p) + np.conj(np.prod(J_p*C_p)))*decay\n",
    "    \n",
    "    \n",
    "def pi1k(W12_list, W21_list, T, t_cycle, V_list, k=1, phi = 0, disp = False):\n",
    "    \n",
    "    pi11 = 0\n",
    "    n = len(W12_list)\n",
    "    C_plus = 2*C_plus0(W12_list, W21_list, T, t_cycle, V_list, k=k)\n",
    "    C_pT = 2*C_plus0T(W12_list, W21_list, T, V_list)\n",
    "    \n",
    "    J_p = J_plus(W12_list, W21_list, T, V_list)    \n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "    if disp:\n",
    "        print(f'W : {W_arr}, dW : {dW_arr}')\n",
    "    idx_list = np.arange(n)[::-1]\n",
    "    idx_set = set(idx_list)\n",
    "    for s in range(n):\n",
    "        K_idx_list = itertools.combinations(idx_list, s+1)\n",
    "        for K_idx in K_idx_list:\n",
    "            n_list = list(idx_set.difference(set(K_idx)))\n",
    "            J_list = [J_p[k] for k in K_idx]\n",
    "            W_list = [W_arr[k] for k in K_idx]\n",
    "            if len(n_list) == 0:\n",
    "                C_p_list = [1]\n",
    "            else:\n",
    "                C_p_list = [C_pT[k] for k in n_list]\n",
    "            C_p_m = [C_plus[k] for k in K_idx]\n",
    "            if disp:\n",
    "                print(f'J : {J_list}, C : {C_p_list}, Cpm : {C_p_m}')\n",
    "                print(f's : {s}, ms : {K_idx}, n : {n_list}')\n",
    "            K = 1/4*np.prod(J_list)*np.prod(C_p_list)*np.exp(1j*phi)\n",
    "            K = K + np.conj(K)\n",
    "            pi11 += 1/2*K*np.prod(C_p_m)*np.prod(C_p_list)\n",
    "        \n",
    "    return np.real(pi11*np.exp(1j*phi))\n",
    "\n",
    "def pi11_list(W12_list, W21_list, T, t_cycle, V_list,  phi = 0, k_max = 100):\n",
    "    '''\n",
    "    This is a function that calculates $\\tilde{r}_2(k)$ - centered correlator according to Eq. (19)\n",
    "    in the paper. The subroutines of that functions are different than the one presented in the paper,\n",
    "    however they give the same results (old notation and derivation convention)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    Pi11_list = [pi1k(W12_list, W21_list, T, t_cycle, V_list, k=k, phi = phi) for k in range(1,k_max)]\n",
    "    return Pi11_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main sampling function that also saves the results for a qubit dispersively coupled to a set of TLSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLS_sampling_PS(N_total, W12_list, W21_list, T, t_cycle, dt, V_list,  phi=0, \n",
    "                          lag_list = [0], reps=1, save = True, k_max = 0, \n",
    "                    avg = True, W_type = '', save_all = True, PS = True,\n",
    "                   batch_size = 0):\n",
    "    '''\n",
    "    Function that simulates, calculates, and saves  TLS evolution\n",
    "    average is required to be true if we want to calculate PS\n",
    "    save_all is also required if additionally we're batching\n",
    "    \n",
    "    Input: \n",
    "    N_total   -- number of Ramsey cycles in one repetition\n",
    "    W12_list  -- switching rates between 1 and 2 (1 and 0 in the paper's notation)\n",
    "    W21_list  -- switching rates between 2 and 1 (0 and 1 in the paper's notation)\n",
    "    T         -- interpulse time (t_R in the paper)\n",
    "    t_cycle   -- interpulse + reset time (t_cyc)\n",
    "    dt        -- discrete time step, at whcih TLSs switching can happen\n",
    "    V_list    -- list of coupling strengths\n",
    "    phi       -- phase shift (detuning)\n",
    "    lag_list  -- a list of \"lags\"/\"delays\" in r_3(k,l) (i.e. fixing value of k or l at the value of a lag)\n",
    "                 if you want to calculate r_3(k, k+3), you select lag_list = [3]. If a list is longer,\n",
    "                 it allows to calcluate more setups (here we fixed lags only to k, easy to change though)\n",
    "    reps      -- how many times an entire sequence of measurements is repeated\n",
    "    save      -- True if we want to save\n",
    "    k_max     -- how many correlation elements we want to compute\n",
    "    avg       -- averages out all frequency fluctuations over TLSs ('vertical' integration over TLS average, contrary\n",
    "                 to a 'horizontal' integration (in time) to obtain theta)\n",
    "    W_type    -- a string for saving, I use here 'sym'/'asym' to indicate if the setup is composed of symmetric\n",
    "                 asymmetric TLSs, but it can be any string that is helpful.\n",
    "    save_all  -- saves all data: all TLSs outcomes, theta, correlations etc. Note that this generates large files\n",
    "    PS        -- if we want to keep track of the empirical power spectrum (FFT)\n",
    "    batch_size -- splits the sampling into the number of batches and than snitches back data together,\n",
    "                 allows to avoid keeping large arrays in the memory.\n",
    "                 \n",
    "    '''\n",
    "    t0 = time.monotonic()\n",
    "    t_stamp = datetime.now()\n",
    "    \n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "    V0 = V_list[0]\n",
    "    theta_list = []\n",
    "    outcomes_list = []\n",
    "    dn_list = []\n",
    "    NTLS = len(W12_list)\n",
    "\n",
    "    tin = time.monotonic()\n",
    "    \n",
    "    \n",
    "    if batch_size == 0:\n",
    "        \n",
    "        theta, outcomes_01, dn = noise_stat_freq(N_total, W12_list, W21_list,T, t_cycle, \n",
    "                                        dt,V_list = V_list, phi = phi, avg = avg, reps = reps )\n",
    "    else:\n",
    "        if save_all:\n",
    "            theta = np.zeros((reps,N_total))\n",
    "            dn = np.zeros((reps, NTLS, N_total))\n",
    "        outcomes_01 = np.zeros((reps, N_total))\n",
    "        for k in range(batch_size):\n",
    "            reps_new = reps//batch_size\n",
    "            if save_all:\n",
    "                theta_temp, outcomes_temp, dn_temp = noise_stat_freq(N_total, W12_list, W21_list,T, t_cycle, \n",
    "                                                dt,V_list = V_list, phi = phi, avg = avg, reps = reps_new )    \n",
    "                theta[k*reps_new:(k+1)*reps_new,:] = theta_temp\n",
    "                if avg:\n",
    "                    dn[k*reps_new:(k+1)*reps_new,:] = dn_temp\n",
    "                else:\n",
    "                    dn[k*reps_new:(k+1)*reps_new,:,:] = dn_temp\n",
    "            else:\n",
    "                _, outcomes_temp, _ = noise_stat_freq(N_total, W12_list, W21_list,T, t_cycle, \n",
    "                                dt,V_list = V_list, phi = phi, avg = avg, reps = reps_new )\n",
    "            outcomes_01[k*reps_new:(k+1)*reps_new,:] = outcomes_temp\n",
    "            print(f'time for batch {k}: {time.monotonic() - tin}')\n",
    "                \n",
    "    \n",
    "    if PS:\n",
    "        m = dn.shape[-1]\n",
    "        Sf = np.abs(fft(dn)*dt)**2 / (m*dt)\n",
    "        Sf = Sf/reps\n",
    "        freq = np.arange(m)/(dt*m)\n",
    "    else:\n",
    "        Sf = 0\n",
    "        freq = 0    \n",
    "\n",
    "\n",
    "    tin = time.monotonic()\n",
    "    if k_max == 0:\n",
    "        k_max = int(np.sqrt(N_total))+10\n",
    "    pi1_sampled = np.mean(outcomes_01)\n",
    "    pi1_theo = pi1_TLS(W12_list, W21_list, T, V_list, phi = phi)\n",
    "\n",
    "    n = len(W12_list)\n",
    "    print(f'pi(1) sampled: {pi1_sampled}, pi(1) theo : {pi1_theo}')\n",
    "    W_min = W_arr[0]\n",
    "    \n",
    "    if W_min == 0.01:\n",
    "        W_idx = '0'\n",
    "    elif W_min == 0.001:\n",
    "        W_idx = '1'\n",
    "    elif W_min == 0.0001:\n",
    "        W_idx = '2'\n",
    "    else :\n",
    "        W_idx = '00'\n",
    "        \n",
    "    if phi == 0:\n",
    "        phi_idx = '0'\n",
    "    elif phi == np.pi/2:\n",
    "        phi_idx = '1'\n",
    "    elif phi == np.pi/4:\n",
    "        phi_idx = '2'\n",
    "    else: \n",
    "        phi_idx = '00'\n",
    "    \n",
    "\n",
    "\n",
    "    if save:\n",
    "        parent = f'./data_TLS'\n",
    "        if not os.path.exists(parent):\n",
    "            os.mkdir(parent)\n",
    "            \n",
    "        path = parent +  f'/data_T={T}_tseq={t_cycle}_W={W_idx}_phi={phi_idx}_N={N_total}_NTLS={NTLS}_V={V0}_alpha={alpha}{W_type}'\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            \n",
    "        specs_dict = {\"N_total\" : N_total, \n",
    "                      \"W12_list\" : W12_list,\n",
    "                     \"W21_list\" : W21_list,\n",
    "                      \"V_list\" : V_list,\n",
    "                     \"T\" :T, \"t_cycle\" : t_cycle, 'dt' : dt, \n",
    "                      'phi' : phi, 'k_max' : k_max,\n",
    "                     'reps' : reps,\n",
    "                     \"N-TLS\" : len(W12_list), \n",
    "                      'pi1_samp' : pi1_sampled, 'pi1_theo' : pi1_theo} \n",
    "\n",
    "        file = open(path + \"/specs.txt\",\"w\")\n",
    "        for key, value in specs_dict.items():     \n",
    "            file.write('%s:%s\\n' % (key, value))\n",
    "        file.close()\n",
    "\n",
    "    Pi11 = np.array(correlation(outcomes_01, N = k_max)).T\n",
    "    if NTLS <= 20:\n",
    "        Pi11_theo = np.array(pi11_list(W12_list, W21_list, T, t_cycle, V_list,  phi = phi, k_max = k_max))\n",
    "    else:\n",
    "        Pi11_theo = [0]\n",
    "    print('time ellapsed C_out: ', time.monotonic() - tin)\n",
    "    \n",
    "    if lag_list[0] != 0 :\n",
    "        C3_list = []\n",
    "        Pi111_list = []\n",
    "        tin3 = time.monotonic()\n",
    "        for lag in lag_list:\n",
    "            C3 = correlation_3_shift(outcomes_01, lag, lag_type = 'k', N = k_max + 15)\n",
    "            C3_list.append(np.mean(C3, axis = 0))\n",
    "\n",
    "            \n",
    "\n",
    "    r2 = np.mean(Pi11, axis = 0)\n",
    "    print(f'r2 shape : {r2.shape}')\n",
    "    if phi != np.pi/2:\n",
    "        exp_f11 = (2*pi1_sampled-1)/np.cos(phi)\n",
    "        if exp_f11 < 0.5:\n",
    "            f11 = 0\n",
    "        else:\n",
    "            f11 = -2*np.log(exp_f11)\n",
    "    elif phi == np.pi/2:\n",
    "        print(\"Can't determine f_0!\")\n",
    "        exp_f11 = 1\n",
    "        f11 = 0\n",
    "\n",
    "    if phi == np.pi/4:\n",
    "        exp_f1k = 8*r2[1:]/(exp_f11**2) + 1\n",
    "        f1k = np.log(exp_f1k)\n",
    "    elif phi == 0:\n",
    "        cosh_f1k = 4*r2[1:]/(exp_f11**2) + 1\n",
    "        f1k = np.arccosh(cosh_f1k)\n",
    "    elif phi == np.pi/2:\n",
    "        sinh_f1k = 4*r2[1:]/(exp_f11**2) \n",
    "        f1k = np.arcsinh(sinh_f1k)\n",
    "\n",
    "    F_row = np.zeros(f1k.shape[-1]+1)\n",
    "    F_row[0] = f11\n",
    "    F_row[1:] = f1k\n",
    "    r3_theo = pi1kl_shift_list(F_row, 3, phi=phi, lag_type = 'k')\n",
    "            \n",
    "    specs_dict = {\"N_total\" : N_total, \n",
    "              \"W12_list\" : W12_list,\n",
    "             \"W21_list\" : W21_list,\n",
    "              \"V_list\" : V_list,\n",
    "             \"T\" :T, \"t_cycle\" : t_cycle, 'dt' : dt, \n",
    "              'phi' : phi, 'k_max' : k_max,\n",
    "             'reps' : reps,\n",
    "             \"N-TLS\" : len(W12_list), \n",
    "              'pi1_samp' : pi1_sampled, 'pi1_theo' : pi1_theo} \n",
    "    \n",
    "    if save_all:\n",
    "        print('save all')\n",
    "        output_dict = {'dn' :dn, 'out': np.array(outcomes_01), 'pi11_samp': Pi11, \n",
    "              'pi11_theo': Pi11_theo, 'pi111' :C3_list, 'theta':theta_list, \n",
    "                   'pi1':[pi1_sampled, pi1_theo], 'specs' : specs_dict, \n",
    "                       'freq' : freq, 'Sf' : Sf, 'pi111_theo' : r3_theo, 'Frow': F_row }\n",
    "    else:\n",
    "        output_dict = {'dn' :0,'out': np.array(outcomes_01), 'pi11_samp': Pi11, \n",
    "              'pi11_theo': Pi11_theo, 'pi111' :C3_list, 'theta':0, \n",
    "                   'pi1':[pi1_sampled, pi1_theo], 'specs' : specs_dict,\n",
    "                       'freq' : freq, 'Sf' : Sf, 'pi111_theo' : r3_theo, 'Frow': F_row}        \n",
    "    if save: \n",
    "\n",
    "        output_dict_to_dump = f'/output_dict.p'\n",
    "        with open(path + output_dict_to_dump, 'wb') as f:\n",
    "            pickle.dump(output_dict, f)  \n",
    "\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar routine as above, but tailored to collect only samples for short sequences, to study non-ergodic behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TLS_sampling_short(N_total, W12_list, W21_list, T, t_cycle, dt, \n",
    "                       V_list,  phi=0, reps=1, save = False, batch_size = 0, stamp = ''):\n",
    "    '''\n",
    "    Function that simulates, calculates, saves and plots TLS evolution\n",
    "    '''\n",
    "    t0 = time.monotonic()\n",
    "    t_stamp = datetime.now()\n",
    "    \n",
    "    W_arr, dW_arr = delta_W(W12_list, W21_list)\n",
    "#     V0 = V_list[0]\n",
    "\n",
    "    if batch_size == 0:\n",
    "        \n",
    "        theta, outcomes_01, dn = noise_stat_freq(N_total, W12_list, W21_list,T, t_cycle, \n",
    "                                        dt,V_list = V_list, phi = phi, avg = True, reps = reps )\n",
    "    else:\n",
    "        if theta_save:\n",
    "            theta = np.zeros((reps,N_total))\n",
    "#             dn = np.zeros((reps, NTLS, N_total))\n",
    "        outcomes_01 = np.zeros((reps, N_total))\n",
    "        for k in range(batch_size):\n",
    "            reps_new = reps//batch_size\n",
    "            if theta_save:\n",
    "                theta_temp, outcomes_temp, dn_temp = noise_stat_freq(N_total, W12_list, W21_list,T, t_cycle, \n",
    "                                                dt,V_list = V_list, phi = phi, avg = True, reps = reps_new )    \n",
    "                theta[k*reps_new:(k+1)*reps_new,:] = theta_temp\n",
    "\n",
    "            else:\n",
    "                _, outcomes_temp, _ = noise_stat_freq(N_total, W12_list, W21_list,T, t_cycle, \n",
    "                                dt,V_list = V_list, phi = phi, avg = True, reps = reps_new )\n",
    "            outcomes_01[k*reps_new:(k+1)*reps_new,:] = outcomes_temp\n",
    "        \n",
    "\n",
    "    print(f'time: {time.monotonic() - t0}')\n",
    "    \n",
    "    if save:\n",
    "        parent = f'./data_TLS_short'\n",
    "        if not os.path.exists(parent):\n",
    "            os.mkdir(parent)\n",
    "    \n",
    "        W_min = W_arr[0]\n",
    "\n",
    "        if W_min == 0.01:\n",
    "            W_idx = '0'\n",
    "        elif W_min == 0.001:\n",
    "            W_idx = '1'\n",
    "        elif W_min == 0.0001:\n",
    "            W_idx = '2'\n",
    "        else :\n",
    "            W_idx = '00'\n",
    "\n",
    "        if phi == 0:\n",
    "            phi_idx = '0'\n",
    "        elif phi == np.pi/2:\n",
    "            phi_idx = '1'\n",
    "        elif phi == np.pi/4:\n",
    "            phi_idx = '2'\n",
    "        else: \n",
    "            phi_idx = '00'\n",
    "    \n",
    "        path = parent +  f'/data_T={T}_tseq={t_cycle}_W={W_idx}_phi={phi_idx}_N={N_total}_NTLS={NTLS}_V={V0}_alpha={alpha}_{stamp}'\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "            \n",
    "        \n",
    "\n",
    "        output_dict_to_dump = f'/outcomes.p'\n",
    "        with open(path + output_dict_to_dump, 'wb') as f:\n",
    "            pickle.dump(outcomes_01, f)  \n",
    "            \n",
    "        specs_dict = {\"N_total\" : N_total, \n",
    "              \"W12_list\" : W12_list,\n",
    "             \"W21_list\" : W21_list,\n",
    "              \"V_list\" : V_list,\n",
    "             \"T\" :T, \"t_cycle\" : t_cycle, 'dt' : dt, \n",
    "              'phi' : phi, 'reps' : reps,\n",
    "             \"N-TLS\" : len(W12_list)} \n",
    "\n",
    "        file = open(path + \"/specs.txt\",\"w\")\n",
    "        for key, value in specs_dict.items():     \n",
    "            file.write('%s:%s\\n' % (key, value))\n",
    "        file.close()        \n",
    "        \n",
    "    return np.array(outcomes_01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to simulate Gaussian noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expcosh(a,b):\n",
    "    return (np.exp(-(a+b)) + np.exp(-a + b))/2\n",
    "\n",
    "def pi1(f11, phi = 0):\n",
    "    return 1/2 * (1 + np.cos(phi) * np.exp(- f11 / 2))\n",
    "\n",
    "def pi11(f11, f1k, phi = 0):\n",
    "    return 1/8 * np.exp(-f11) * (np.exp(f1k) - 1 - np.cos(2*phi) * (1-np.exp(-f1k)))\n",
    "\n",
    "def pi111(f11, f12, f13 ):\n",
    "    return pi11(f11, f12) + 1/2 * pi11(f11,f13) -3/4 * pi1(f11) + 1/8 * (1 + 1/2 * np.exp(-3/2 * f11) * (expcosh(f13, 2*f12) + np.exp(f13)))\n",
    "\n",
    "\n",
    "def full_pi11(F_row, phi = 0):\n",
    "    f11 = F_row[0]\n",
    "    N = len(F_row)\n",
    "    Pi11 = [pi11(f11, F_row[k], phi = phi) for k in range(1,N)]\n",
    "    return Pi11\n",
    "\n",
    "\n",
    "def pi1kl_f(f1k, f1l, fkl, f11, phi = 0):\n",
    "\n",
    "    pi_1 = pi1(f11, phi = phi)\n",
    "    pi1k = pi11(f11, f1k, phi = phi)\n",
    "    pi1l = pi11(f11, f1l, phi = phi)\n",
    "    pikl = pi11(f11, fkl, phi = phi)\n",
    "    Q1 = (np.exp(-f1k -f1l -fkl) - 1)* np.cos(3*phi)\n",
    "    Q2 = np.cos(phi) * (np.exp(-f1k+f1l+fkl) + np.exp(-f1l +f1k +fkl) + np.exp(f1k+f1l-fkl) -3)\n",
    "    return 1/32 * np.exp(-3/2 * f11) * (Q1+Q2)\n",
    "\n",
    "def pi1kl_f_list(F_row,  lag, phi = 0, lag_type = 'k'):\n",
    "    N_samp = len(F_row)\n",
    "    f11 = F_row[0]\n",
    "    if lag_type == 'k':\n",
    "        C_out = [pi1kl_f(F_row[lag], F_row[lag+k], F_row[k], f11, phi = phi) for k in range(1,N_samp-lag)]\n",
    "    elif lag_type == 'l':\n",
    "        C_out = [pi1kl_f(F_row[k], F_row[lag+k], F_row[lag], f11, phi = phi) for k in range(1,N_samp-lag)]\n",
    "    return C_out\n",
    "\n",
    "def pi1kl_shift(f1k, f1l, fkl, f11, phi = 0):\n",
    "    Q1 = np.cos(phi) * (np.exp(-f1k+f1l+fkl) + np.exp(-f1l +f1k +fkl) + np.exp(f1k+f1l-fkl) +\n",
    "                        6-2*np.exp(f1k)-2*np.exp(f1l)-2*np.exp(fkl)-\n",
    "                        np.exp(-f1k)-np.exp(-f1l)-np.exp(-fkl))\n",
    "    Q2 = np.cos(3*phi)*(np.exp(-f1k -f1l -fkl) +2-np.exp(-f1k)-np.exp(-f1l)-np.exp(-fkl) )\n",
    "    return 1/32 * np.exp(-3/2 * f11) * (Q1+Q2)\n",
    "def pi1kl_shift_list(F_row, lag, phi=0, lag_type = 'k'):\n",
    "    N_samp = len(F_row)\n",
    "    f11 = F_row[0]\n",
    "    if lag_type == 'k':\n",
    "        C_out = [pi1kl_shift(F_row[lag], F_row[lag+k], F_row[k], f11, phi = phi) for k in range(1,N_samp-lag)]\n",
    "    elif lag_type == 'l':\n",
    "        C_out = [pi1kl_shift(F_row[k], F_row[lag+k], F_row[lag], f11, phi = phi) for k in range(1,N_samp-lag)]\n",
    "    return C_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix $f_k=\\langle \\theta_0\\theta_k\\rangle$ for exponentially correlated noise. Here we use slightly different representation that is currently in the paper. However, easy to map one to another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnm(T, D, tn, tm):\n",
    "    return D * (np.cosh(T)-1) * np.exp(-abs(tn - tm))\n",
    "\n",
    "def f_one_one(T, D):    \n",
    "    return D * (T - (1-np.exp(-T)))\n",
    "\n",
    "\n",
    "def F_matrix(N, T, D, t_cycle):\n",
    "    f11 = f_one_one(T,D)\n",
    "    F = np.array([[fnm(T, D,k * t_cycle, l * t_cycle)  if k != l else f11 for k in range(N)] for l in range(N)])\n",
    "    return F\n",
    "\n",
    "def Fnm_row(N, T, D, t_cycle):\n",
    "    f11 = f_one_one(T,D)\n",
    "    F = np.array([fnm(T,D,0,k*t_cycle) if k != 0 else f11 for k in range(N)])\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation function for $1/f$ noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fk_approx(N, D, T, t_cycle, omega):\n",
    "    gamma = np.euler_gamma\n",
    "    k_start = 150\n",
    "    k_list = np.arange(1,N+1)\n",
    "    fk = D*T**2/(np.pi)*(-gamma - np.log(k_list*omega*t_cycle))\n",
    "    return fk\n",
    "\n",
    "def f0_new(T, D, omega):\n",
    "    b = omega * T\n",
    "    shi, chi = shichi(b)\n",
    "    return T**2 * D/np.pi * (-1/b**2 + 2/b- (b-1)*np.exp(-b)/b**2 - chi+shi )\n",
    "\n",
    "def f_one_one_arctan(T, D, omega_min):\n",
    "    b = omega_min * T   \n",
    "    return D * 1/2 * (-(1/b**2) + 2/b - ((-1 + b) * np.exp(-b))/b**2 \n",
    "                     - shichi(b)[1] +    shichi(b)[0])\n",
    "    \n",
    "\n",
    "def fnm_arctan(T, D, omega_min, t_cycle, n):\n",
    "    a = n*t_cycle/T\n",
    "    b = omega_min * T\n",
    "\n",
    "    return D*T**2/(2*np.pi)* (2 * a**2 * expi(-a * b) + 1/b**2*np.exp(-a * b) * \n",
    "                       (-2 * (-1 + a * b) * (-1 + np.cosh(b))  \n",
    "          - b**2 * np.exp(a * b) * ((1 + a)**2 * expi(-((1 + a) * b)) + (-1 + \n",
    "            a)**2 * expi(b - a * b)) + 2 * b * np.sinh(b)))\n",
    "\n",
    "def F_matrix_arctan(N, T, D, t_cycle, omega_min):\n",
    "    f11 = f0_new(T, D, omega_min)\n",
    "    F = np.array([[fnm_arctan(T, D, omega_min,  t_cycle, abs(k-l))  if k != l else f11 for k in range(N)] for l in range(N)])\n",
    "    return F\n",
    "\n",
    "def Fnm_row_arctan(N,  T , D, t_cycle, omega_min):\n",
    "    f11 = f0_new(T, D, omega_min)\n",
    "    F = np.array([fnm_arctan( T, D, omega_min,  t_cycle, k) if k != 0 else f11 for k in range(N)])\n",
    "    return F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling function of random phases. First for exponentially correlated, then for 1/f noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_theta(N_samples, T, D,  t_cycle,  n_corr, reps):\n",
    "    \n",
    "\n",
    "    F = F_matrix(n_corr+1, T, D,  t_cycle)\n",
    "\n",
    "    for N in range(2,n_corr):\n",
    "        k = N-1\n",
    "        F_old = F[:k,:k]\n",
    "        Psi_old = np.linalg.inv(F_old)\n",
    "        M = Psi_old[-1]\n",
    "        if np.any(abs(M)  < 1e-4):\n",
    "            n_corr = N\n",
    "            break\n",
    "    if n_corr <=2 :\n",
    "        n_corr = 2\n",
    "    print(f'n_corr : {n_corr}')\n",
    "    tin = time.monotonic()\n",
    "    F = F_matrix(n_corr+1, T, D,  t_cycle)\n",
    "    Psi = np.linalg.inv(F)\n",
    "    print(f'F shape = {F.shape}')\n",
    "    sigma = np.sqrt(1/Psi[0,0])\n",
    "\n",
    "    \n",
    "    theta_1 = np.random.normal(loc = 0, scale = sigma, size = (1, reps))\n",
    "    theta_arr = np.zeros(( reps, N_samples))\n",
    "    theta_arr[:,0] = theta_1\n",
    "    \n",
    "    sigma_list = [sigma]\n",
    "    mu_list = []\n",
    "    for k in range(1,N_samples):\n",
    "        \n",
    "        if k <= n_corr-1:\n",
    "            Psi_temp = np.linalg.inv(F[:k+1,:k+1])\n",
    "            sigma = np.sqrt(1/Psi_temp[k,k])\n",
    "            mu = -np.sum(Psi_temp[k,:k]*theta_arr[:,:k], axis = 1)/Psi_temp[k,k]\n",
    "            psi_theta = Psi_temp[k,:k]*theta_arr[:,:k]\n",
    "            theta_tmp = np.random.normal(loc = mu, scale = sigma, size = (1,reps))\n",
    "            theta_arr[:,k] = theta_tmp\n",
    "            mu_list.append(mu)\n",
    "\n",
    "        else:\n",
    "            mu = -np.sum(Psi[n_corr,-n_corr-1:n_corr]*theta_arr[:,k-n_corr:k], axis = 1)/Psi[n_corr,n_corr]\n",
    "            sigma = np.sqrt(1/Psi[n_corr,n_corr])\n",
    "            theta_tmp = np.random.normal(loc = mu, scale = sigma, size = (1,reps))\n",
    "            theta_arr[:,k] = theta_tmp\n",
    "            mu_list.append(mu)\n",
    "    return theta_arr, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_theta_flicker(N_samples, T, D,  t_cycle, omega,  n_corr, reps, \n",
    "                           noise = 'int', progress = False,Gamma = 0.0005):\n",
    "    '''\n",
    "    noise parameter, Gamma are obsolete\n",
    "    '''\n",
    "    \n",
    "    rt = t_cycle/T\n",
    "\n",
    "    F = F_matrix_arctan(n_corr+1, T, D, t_cycle, omega)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'n_corr : {n_corr}')\n",
    "    tin = time.monotonic()\n",
    "\n",
    "    Psi = np.linalg.inv(F)\n",
    "    print(f'F shape = {F.shape}')\n",
    "    sigma = np.sqrt(1/Psi[0,0])\n",
    "\n",
    "    theta_1 = np.random.normal(loc = 0, scale = sigma, size = (1, reps))\n",
    "    theta_arr = np.zeros(( reps, N_samples))\n",
    "    theta_arr[:,0] = theta_1\n",
    "    \n",
    "    sigma_list = [sigma]\n",
    "    mu_list = []\n",
    "    tin = time.monotonic()\n",
    "    for k in range(1,N_samples):\n",
    "        \n",
    "        if k <= n_corr-1:\n",
    "            Psi_temp = np.linalg.inv(F[:k+1,:k+1])\n",
    "            sigma = np.sqrt(1/Psi_temp[k,k])\n",
    "            mu = -np.sum(Psi_temp[k,:k]*theta_arr[:,:k], axis = 1)/Psi_temp[k,k]\n",
    "            psi_theta = Psi_temp[k,:k]*theta_arr[:,:k]\n",
    "            theta_tmp = np.random.normal(loc = mu, scale = sigma, size = (1,reps))\n",
    "            theta_arr[:,k] = theta_tmp\n",
    "            mu_list.append(mu)\n",
    "\n",
    "        else:\n",
    "            mu = -np.sum(Psi[n_corr,-n_corr-1:n_corr]*theta_arr[:,k-n_corr:k], axis = 1)/Psi[n_corr,n_corr]\n",
    "            sigma = np.sqrt(1/Psi[n_corr,n_corr])\n",
    "            theta_tmp = np.random.normal(loc = mu, scale = sigma, size = (1,reps))\n",
    "            theta_arr[:,k] = theta_tmp\n",
    "            mu_list.append(mu)\n",
    "        if progress:\n",
    "            if k % 2500 == 0:\n",
    "                print(f'k = {k}, t : {time.monotonic() - tin}')\n",
    "    return theta_arr, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_sampling(N_samples, T, D, t_cycle, lag = 5, \n",
    "                  save = False, reps = 100, k_max = 0,  \n",
    "                  n_corr = 20, N = 0, save_all = False, phi_list = [0, np.pi/4, np.pi/2]):\n",
    "    \n",
    "    specs_dict = {\"T\" : T, \"N_samp\" : N_samples, \"D\" : D, \"t_cycle\" : t_cycle, 'lag' : lag,\n",
    "                 'reps' : reps, 'k_max' : k_max, 'n_corr' : n_corr, 'N' : N}\n",
    "    \n",
    "    t0 = time.monotonic()\n",
    "    theta_dict = {}\n",
    "    outcome_dict = {}\n",
    "    Pi1_dict = {}\n",
    "    Pi11_dict = {}\n",
    "    Pi111_dict = {}\n",
    "    Pi111_shift_dict = {}\n",
    "    \n",
    "    Pi1_theo_dict = {}\n",
    "    Pi11_theo_dict = {}\n",
    "    Pi111_theo_dict = {}\n",
    "    Pi111_shift_theo_dict = {}\n",
    "\n",
    "    F_predicted_dict = {}\n",
    "\n",
    "\n",
    "    \n",
    "    theta_arr, F = sampling_theta(N_samples = N_samples, T = T, D = D,\n",
    "                                                t_cycle = t_cycle, n_corr =  n_corr,reps = reps)\n",
    "    \n",
    "    if k_max == 0:\n",
    "        k_max = int(np.sqrt(N_samples))\n",
    "    \n",
    "    for idx, phi in enumerate(phi_list):\n",
    "        \n",
    "        rnd = np.random.rand(reps, N_samples)\n",
    "        \n",
    "        out = (1/2 * (1 + np.cos(phi + theta_arr)) >= rnd) * 1\n",
    "        \n",
    "        outcome_dict[idx] = out\n",
    "        \n",
    "        tin = time.monotonic()\n",
    "        f11 = F[0,0]\n",
    "        F_row = Fnm_row(N+10,T, D, t_cycle)\n",
    "\n",
    "        \n",
    "        Pi1_full = np.mean(out, axis = 1).reshape(-1,1)  \n",
    "        Pi1 = np.mean(out)\n",
    "        Pi1_theo = pi1(f11, phi)\n",
    "        Pi1_dict[idx] = [Pi1, Pi1_full]\n",
    "        Pi1_theo_dict[idx] = Pi1_theo\n",
    "        \n",
    "        Pi11 = (np.array(correlation(out, N)).T)[:,1:]\n",
    "        Pi11_theo = full_pi_11(T,  D,  t_cycle = t_cycle, phi = phi,  k_max = N_samples)[1:]\n",
    "        Pi11_dict[idx] = Pi11\n",
    "        Pi11_theo_dict[idx] = Pi11_theo\n",
    "\n",
    "        \n",
    "        Pi111_dict[idx] = [0]\n",
    "        Pi111_theo_dict[idx] =[0]\n",
    "        \n",
    "        Pi111_shift = correlation_3_shift(out, lag, lag_type = 'k', N = N+10)\n",
    "        Pi111_shift_theo = pi1kl_shift_list(F_row, lag, phi, lag_type = 'k')\n",
    "        \n",
    "        Pi111_shift_dict[idx] = Pi111_shift\n",
    "        Pi111_shift_theo_dict[idx] = Pi111_shift_theo\n",
    "        \n",
    "\n",
    "            \n",
    "        print(f'idx : {idx} time : {time.monotonic() - t0}')\n",
    "    if save_all:\n",
    "        output = (outcome_dict, 0, Pi1_dict, Pi1_theo_dict, \n",
    "                  Pi11_dict, Pi11_theo_dict, Pi111_dict, Pi111_theo_dict, \n",
    "                  Pi111_shift_dict, Pi111_shift_theo_dict,\n",
    "                 F_predicted_dict, F, specs_dict)\n",
    "    else:\n",
    "        output = (0, 0, Pi1_dict, Pi1_theo_dict, \n",
    "          Pi11_dict, Pi11_theo_dict, Pi111_dict, Pi111_theo_dict, \n",
    "          Pi111_shift_dict, Pi111_shift_theo_dict,\n",
    "         0, 0, specs_dict)\n",
    "    return output\n",
    "        \n",
    "    \n",
    "def full_sampling_flicker(N_samples, T, D, t_cycle, omega, lag = 5, \n",
    "              save = False, reps = 100, k_max = 0,  n_corr = 20,\n",
    "                          N = 0, noise = 'int', save_all = False, \n",
    "                          phi_list = [0, np.pi/4, np.pi/2], Gamma = 0.005):\n",
    "\n",
    "\n",
    "    specs_dict = {\"T\" : T, \"N_samp\" : N_samples, \"D\" : D, \"t_cycle\" : t_cycle, 'lag' : lag,\n",
    "                 'reps' : reps, 'k_max' : k_max, 'n_corr' : n_corr, 'N' : N, 'omega' : omega,\n",
    "                 'noise' : noise}\n",
    "\n",
    "    t0 = time.monotonic()\n",
    "    theta_dict = {}\n",
    "    outcome_dict = {}\n",
    "    Pi1_dict = {}\n",
    "    Pi11_dict = {}\n",
    "    Pi111_dict = {}\n",
    "    Pi111_shift_dict = {}\n",
    "\n",
    "    Pi1_theo_dict = {}\n",
    "    Pi11_theo_dict = {}\n",
    "    Pi111_theo_dict = {}\n",
    "    Pi111_shift_theo_dict = {}\n",
    "    \n",
    "    F_predicted_dict = {}\n",
    "\n",
    "    \n",
    "\n",
    "    theta_arr, F = sampling_theta_flicker(N_samples = N_samples, T = T, D = D,\n",
    "                                                t_cycle = t_cycle, omega = omega,\n",
    "                                          n_corr =  n_corr,reps = reps, \n",
    "                                          noise = noise, Gamma = Gamma)\n",
    "    if k_max == 0:\n",
    "        k_max = int(np.sqrt(N_samples))\n",
    "    print(f'time for theta : {time.monotonic() - t0}, k_max : {k_max}')\n",
    "    rt = t_cycle/T\n",
    "    \n",
    "    F_row = F[0]\n",
    "\n",
    "    for idx, phi in enumerate(phi_list):\n",
    "\n",
    "        rnd = np.random.rand(reps, N_samples)\n",
    "\n",
    "        out = (1/2 * (1 + np.cos(phi + theta_arr)) >= rnd) * 1\n",
    "\n",
    "        outcome_dict[idx] = out\n",
    "\n",
    "        tin = time.monotonic()\n",
    "        f11 = F[0,0]\n",
    "        if noise == 'color':\n",
    "            F_row = Fnm_row_color(N+10, T, D, t_cycle, omega, Gamma)\n",
    "        else:\n",
    "            F_row = Fnm_row_arctan(N+10,T, D, t_cycle, omega )\n",
    "\n",
    "\n",
    "        Pi1_full = np.mean(out, axis = 1).reshape(-1,1)  \n",
    "        Pi1 = np.mean(out)\n",
    "        Pi1_theo = pi1(f11, phi)\n",
    "        Pi1_dict[idx] = [Pi1, Pi1_full]\n",
    "        Pi1_theo_dict[idx] = Pi1_theo\n",
    "        \n",
    "        print(f'time for Pi1 : {time.monotonic() - tin}')\n",
    "        \n",
    "        Pi11 = (np.array(correlation(out, N)).T)[:,1:]\n",
    "        print(f'time for Pi11 samp : {time.monotonic() - tin}')\n",
    "        Pi11_theo = full_pi11(F_row, phi = phi)\n",
    "        Pi11_dict[idx] = Pi11\n",
    "        Pi11_theo_dict[idx] = Pi11_theo\n",
    "        print(f'time for Pi11 total: {time.monotonic() - tin}')\n",
    "\n",
    "        Pi111_shift = correlation_3_shift(out, lag, lag_type = 'k', N = N+10)\n",
    "        Pi111_shift_theo = pi1kl_shift_list(F_row, lag, phi, lag_type = 'k')\n",
    "        \n",
    "        Pi111_shift_dict[idx] = Pi111_shift\n",
    "        Pi111_shift_theo_dict[idx] = Pi111_shift_theo\n",
    "\n",
    "    if save_all:\n",
    "        output = (outcome_dict, 0, Pi1_dict, Pi1_theo_dict, \n",
    "                  Pi11_dict, Pi11_theo_dict, 0, 0, \n",
    "                  Pi111_shift_dict,Pi111_shift_theo_dict,\n",
    "                 0, 0, specs_dict)\n",
    "    else:\n",
    "        output = (0, 0, Pi1_dict, Pi1_theo_dict, \n",
    "                  Pi11_dict, Pi11_theo_dict, 0, 0, \n",
    "                  Pi111_shift_dict,Pi111_shift_theo_dict,\n",
    "                 0, 0, specs_dict)\n",
    "    return output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling function for non-ergodic case. We can specify noise type to either sample exponentially correlated or 1/f noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_flicker_short(N_samples, T, D, t_cycle, omega, reps = 100, \n",
    "                           n_corr = 20,phi = np.pi/4, noise_type = 'flicker'):\n",
    "    '''\n",
    "    Maybe we will adjust correlation matrix, to get rid of tau, and give other meaning to\n",
    "    the used parameters\n",
    "    '''\n",
    "    \n",
    "    if noise_type == 'flicker':\n",
    "        theta_arr, F = sampling_theta_flicker(N_samples = N_samples, T = T, D = D,\n",
    "                                                    t_cycle = t_cycle, omega = omega,\n",
    "                                              n_corr =  n_corr,reps = reps, noise = 'arctan')\n",
    "    else:\n",
    "        theta_arr, F = sampling_theta(N_samples = N_samples, T = T, D = D,t_cycle = t_cycle, \n",
    "                                      n_corr =  n_corr,reps = reps)        \n",
    "    rnd = np.random.rand(reps, N_samples)\n",
    "\n",
    "    output = (1/2 * (1 + np.cos(phi + theta_arr)) >= rnd) * 1\n",
    "    return output, theta_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
